---
title: "Subsampling your recordings"
output: 
  rmarkdown::html_vignette:
    code_folding: s

description: >
  This article covers the workflow to randomly sample recordings.
vignette: >
  %\VignetteIndexEntry{Subsampling recordings}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(ARUtools)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(tibble.print_min = 4L, tibble.print_max = 4L)
```

This vignette will walk through the workflow of subsampling recordings. This
can be particularily useful if you have many more recordings than you can 
interpret manually.




## Create data

We will generate some data from the sites in `example_sites`. Click on the triangle
below to see the details of this method.

<!-- https://github.com/mrworthington/mrworthington.github.io/blob/master/articles/public_health/covid_atx/covid_atx_analysis.qmd -->
<details> <summary> <strong>Details of simulation </strong> </summary>


To simulate the file names we generate a series of recordings at each site. 
Here the schedule is every 30 minutes between 5:30 and 8:00 AM every second 
day between 1 May and 10 July. 

Normally you would want to schedule based around local sunrise if targeting the 
dawn chorus, but this will work for our purposes.

```{r, code_folding = 'hide'}
library(dplyr)
library(stringr)
library(lubridate)

simple_deploy <- 
tidyr::expand_grid(site_id = unique(example_sites$Sites),
                   
                   doy = seq(121,191, by = 2),
                   times = seq(-30, 120, by = 30)) |> 
  tidyr::separate(site_id, into = c("plot", "site"), sep = "_", remove = F) |> 
  left_join(example_sites, join_by(site_id == Sites)) |> 
  mutate(
    # aru_id = glue::glue("BARLT-000{as.numeric(as.factor(site_id))}"),
    date = ymd("2028-01-01")+doy,
         date_time = ymd_hm(glue::glue("{date} 06:00"))+minutes(times),
         date_time_chr = str_replace(as.character(date_time), "\\s", "T"),
         file_name = glue::glue("{plot}/{site_id}/{ARU}_{date_time_chr}.wav") )
  
simple_deploy

```

Our site info will be that used in `example_sites`.

```{r}
site_info <- simple_deploy |> 
  slice_min(order_by = date_time, n=1, by = site_id) |> 
  dplyr::select(site_id, ARU, lon, lat, date_time)
  
```

</details>

## Clean metadata

To clean the metadata we can use the same code found in `vignette("ARUtools")`. 
I have used a pipe to save space here, but you can see the details in the 
`vignette("ARUtools")`.

```{r}
sites <- clean_site_index(site_info,
                          col_aru_id = "ARU", 
                          col_site_id = "site_id", 
                          col_date_time = c("date_time"),
                          col_coords = c("lon", "lat") )
metadata <- clean_metadata(project_files = simple_deploy$file_name) |> 
  add_sites( sites) |> 
  calc_sun() |> 
  dplyr::mutate( doy = lubridate::yday(date))
dplyr::glimpse(metadata)
```
## Parameters

Generally for random sampling you may not want to have recordings selected with
equal weight across time and dates. For example if you are sampling songbirds in 
the breeding season, most species will be most active for a couple hours from
around sunrise. You will also want to limit the dates to ensure you are picking 
up breeding birds and not migrants.

To deal with this issue we allow the user to specify selection weights based on
the time to sunrise (or sunset) as well as the day of year. 

**Note:** This is an active area of development and you can expect the process of
assigning variables to be much simpler in the next update.


```{r}

params <- list(min_range = c(-70, 240),
                          doy_range = c(120, lubridate::yday(lubridate::ymd("2021-07-20"))),
                          mean_min = 30, sd_min = 60,
                          mean_doy = lubridate::yday(lubridate::ymd("2021-06-10")),
                          sd_doy = 20,off=0,
                          log_ = TRUE, fun = "norm") 
```



To visualize the selection parameters use `gen_dens_sel_simulation`. This will show you how the sample weights 
will change over time and time to sunrise/sunset.

```{r fig.height=6, out.height=8, warning=FALSE}
gen_dens_sel_simulation(parms = params) 
```

Once you have your parameters set up, you can use them to calculate the sampling weights from the metadata.

```{r}



full_selection_probs <- 
  metadata |> 
  calc_sel_pr(
    ARU_ID_col = site_id, 
    min_col = t2sr, 
    day_col = doy,
    parms = params)

```


Below you can see the selection weights `psel_normalized` 


```{r, echo=FALSE}
library(ggplot2)
ggplot(full_selection_probs, aes(doy, t2sr, colour = psel_normalized)) + 
  geom_point() +
  scale_colour_viridis_c() + facet_wrap(~site_id)
```

```{r}
sample_size <- count(full_selection_probs,  site_id) |> 
  transmute(site_id,
            N = floor(n*.02),
         n_os  = ceiling(N*.3)
         )
```



```{r}



# Select Samples
grts_res <- fun_aru_samp(full_selection_probs, 
                         N = sample_size,
                         strat_ = "site_id",
                         seed = 2024, 
                         selprob_id = "psel_normalized",
                         x = 'doy',y = 't2sr')

grts_res$sites_base
```

